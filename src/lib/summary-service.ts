import { GoogleGenerativeAI, type GenerativeModel } from "@google/generative-ai";
import { createAdminClient } from "@/lib/supabase/admin";
const supabase = createAdminClient();
import { transcribeFromUrl, formatTranscriptWithTimestamps, formatTranscriptWithSpeakerNames } from "./deepgram";
import { isVoxtralSupported, transcribeWithVoxtral } from "./voxtral";
import { getAppleTranscript } from "./apple-transcripts";
import type { DiarizedTranscript } from "@/types/deepgram";
import type {
  SummaryLevel,
  SummaryStatus,
  TranscriptStatus,
  QuickSummaryContent,
  DeepSummaryContent
} from "@/types/database";

const genAI = new GoogleGenerativeAI(process.env.GOOGLE_GEMINI_API_KEY!);

// Cached model instances (Fix 2: avoid re-creating on every call)
let flashModel: GenerativeModel | null = null;
let proModel: GenerativeModel | null = null;

function getFlashModel(): GenerativeModel {
  if (!flashModel) {
    flashModel = genAI.getGenerativeModel({
      model: "gemini-3-flash-preview",
      generationConfig: {
        responseMimeType: "application/json"
      }
    });
  }
  return flashModel;
}

function getProModel(): GenerativeModel {
  if (!proModel) {
    proModel = genAI.getGenerativeModel({
      model: "gemini-3.1-pro-preview",
      generationConfig: {
        responseMimeType: "application/json"
      }
    });
  }
  return proModel;
}

// Model selection: Flash for Quick, Pro for Deep analysis
function getModelForLevel(level: SummaryLevel) {
  if (level === 'deep') {
    return getProModel();
  } else {
    return getFlashModel();
  }
}

// Pre-compiled regex patterns for transcript parsing (Fix 3: avoid recompilation in loops)
const SRT_SEQUENCE_RE = /^\d+$/;
const SRT_TIMESTAMP_RE = /^\d{2}:\d{2}:\d{2}[,\.]\d{3}\s*-->\s*\d{2}:\d{2}:\d{2}[,\.]\d{3}/;
const VTT_CUE_ID_RE = /^[\w-]+$/;
const VTT_TIMESTAMP_RE = /^\d{2}:\d{2}[:\.]?\d{0,2}[,\.]?\d{0,3}\s*-->\s*\d{2}:\d{2}[:\.]?\d{0,2}[,\.]?\d{0,3}/;
const VTT_VOICE_TAG_RE = /<v\s+[^>]+>/gi;
const VTT_VOICE_CLOSE_RE = /<\/v>/gi;
const VTT_CLASS_TAG_RE = /<c[^>]*>/gi;
const VTT_CLASS_CLOSE_RE = /<\/c>/gi;
const VTT_ANY_TAG_RE = /<[^>]+>/g;
const MULTI_SPACE_RE = /\s+/g;

import { createLogger } from "@/lib/logger";
import { SUMMARY_STATUS_PRIORITY } from "@/lib/status-utils";
import { triggerPendingNotifications } from "@/lib/notifications/trigger";
import { getCached, setCached, deleteCached, CacheKeys, CacheTTL } from "@/lib/cache";

const logWithTime = createLogger('SUMMARY-SERVICE');

const isDev = process.env.NODE_ENV === 'development';

/**
 * Attempt to repair common JSON issues from LLM output:
 * - Trailing commas before } or ]
 * - Unescaped control characters inside strings (newlines, tabs)
 * - Unescaped quotes inside strings (best-effort)
 */
function repairJsonString(text: string): string {
  let result = text;

  // 1. Remove trailing commas before } or ] (with optional whitespace)
  result = result.replace(/,(\s*[}\]])/g, '$1');

  // 2. Fix unescaped control characters inside JSON strings.
  //    Walk through the string tracking whether we're inside a JSON string value,
  //    and escape raw newlines/tabs that appear inside.
  const chars: string[] = [];
  let inString = false;
  let escaped = false;

  for (let i = 0; i < result.length; i++) {
    const ch = result[i];

    if (escaped) {
      chars.push(ch);
      escaped = false;
      continue;
    }

    if (ch === '\\' && inString) {
      chars.push(ch);
      escaped = true;
      continue;
    }

    if (ch === '"') {
      inString = !inString;
      chars.push(ch);
      continue;
    }

    // If inside a string, escape raw control characters
    if (inString) {
      if (ch === '\n') {
        chars.push('\\n');
        continue;
      }
      if (ch === '\r') {
        chars.push('\\r');
        continue;
      }
      if (ch === '\t') {
        chars.push('\\t');
        continue;
      }
    }

    chars.push(ch);
  }

  return chars.join('');
}

// Speaker identification types
export interface IdentifiedSpeaker {
  id: number;
  name: string;
  role: 'host' | 'guest' | 'unknown';
}

const SPEAKER_ID_PROMPT = `Analyze this podcast transcript and identify the speakers.

Return ONLY a JSON object with this structure:
{
  "speakers": [
    { "id": 0, "name": "John Smith", "role": "host" },
    { "id": 1, "name": "Sarah Johnson", "role": "guest" }
  ]
}

RULES:
- Look for introductions: "Hi, I'm...", "Welcome to...", "Thanks for having me...", "My name is..."
- Look for names mentioned: "Thanks John", "So Sarah, tell us...", "As Mike said..."
- Role "host" = person who welcomes, introduces, asks questions
- Role "guest" = person being interviewed, sharing expertise
- Role "unknown" = can't determine
- If no name found, use descriptive names like "Host", "Guest", "Interviewer", "Expert"
- IMPORTANT: If the transcript is in Hebrew/Spanish/etc., names should still be extracted in their original form
- Always return valid JSON starting with { and ending with }

Transcript sample:
`;

/**
 * Use Claude to identify speaker names from transcript
 */
export async function identifySpeakers(transcript: DiarizedTranscript): Promise<IdentifiedSpeaker[]> {
  logWithTime('identifySpeakers starting', { 
    speakerCount: transcript.speakerCount,
    utteranceCount: transcript.utterances.length 
  });

  const startTime = Date.now();

  // Sample the beginning of the transcript (first 5 minutes) where introductions usually happen
  // Plus some from the middle and end for context
  const fiveMinutes = 5 * 60;
  const beginningUtterances = transcript.utterances.filter(u => u.start < fiveMinutes);
  
  // Also get some samples from middle (for name mentions)
  const middleStart = transcript.duration / 3;
  const middleEnd = (transcript.duration / 3) * 2;
  const middleUtterances = transcript.utterances
    .filter(u => u.start >= middleStart && u.start < middleEnd)
    .slice(0, 20);

  const sampleUtterances = [...beginningUtterances, ...middleUtterances];
  
  // Format for Claude
  const formattedSample = sampleUtterances
    .map(u => `[Speaker ${u.speaker}]: ${u.text}`)
    .join('\n')
    .substring(0, 15000); // Limit to ~15k chars

  try {
    const systemPrompt = "You are a JSON-only response bot. Return ONLY valid JSON.";
    const fullPrompt = systemPrompt + "\n\n" + SPEAKER_ID_PROMPT + formattedSample;
    
    // Use cached Flash model for speaker identification (fast, cheap task)
    const speakerModel = getFlashModel();

    const result = await speakerModel.generateContent(fullPrompt);
    const response = result.response;
    const text = response.text();

    // Parse JSON
    let jsonText = text.trim();
    if (!jsonText.startsWith('{')) {
      const match = jsonText.match(/\{[\s\S]*\}/);
      if (match) jsonText = match[0];
      else throw new Error('No JSON found');
    }

    const parsed = JSON.parse(jsonText);
    const speakers: IdentifiedSpeaker[] = (parsed.speakers || []).map((s: { id: number; name: string; role: string }) => ({
      id: s.id,
      name: s.name || `Speaker ${s.id}`,
      role: (['host', 'guest', 'unknown'].includes(s.role) ? s.role : 'unknown') as IdentifiedSpeaker['role'],
    }));

    const duration = Date.now() - startTime;
    logWithTime('identifySpeakers completed', { 
      durationMs: duration,
      identifiedSpeakers: speakers.map(s => ({ id: s.id, name: s.name, role: s.role }))
    });

    return speakers;
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error);
    logWithTime('identifySpeakers FAILED', { error: errorMsg });
    
    // Return default speaker names on failure
    const defaultSpeakers: IdentifiedSpeaker[] = [];
    for (let i = 0; i < transcript.speakerCount; i++) {
      defaultSpeakers.push({
        id: i,
        name: i === 0 ? 'Host' : `Guest ${i}`,
        role: i === 0 ? 'host' : 'guest'
      });
    }
    return defaultSpeakers;
  }
}

// System message to enforce JSON-only responses
const SYSTEM_MESSAGE = `You are a JSON-only response bot. You MUST respond with ONLY valid JSON - no explanations, no markdown, no text before or after the JSON. Start your response with { and end with }.

CRITICAL JSON RULES:
1. Escape all double quotes inside string values with backslash: \\"
2. Use \\n for newlines inside strings, never raw newlines
3. No trailing commas after the last item in arrays or objects
4. Never say "Based on" or any other text outside the JSON

CRITICAL: You MUST detect the language of the transcript and respond in THE SAME LANGUAGE. This works for ANY language - Hebrew, Spanish, French, German, Japanese, Arabic, Portuguese, Russian, Chinese, or any other. Match the transcript language exactly.`;

// QUICK summary prompt - returns QuickSummaryContent JSON
const QUICK_PROMPT = `You are a senior editor at a top-tier media outlet (like The Economist or TechCrunch).
Your goal is to write a "Teaser Card" that compels the user to consume the full content.

Return ONLY a JSON object with this exact structure:

{
  "hook_headline": "A punchy, provocative 5-10 word headline that captures the essence. NOT 'Summary of episode'.",

  "executive_brief": "2-3 sharp sentences (max 60 words). Don't describe *what* they talked about, describe the *insight* revealed. Start directly with the core conflict or idea.",

  "golden_nugget": "The single most surprising or valuable fact/quote from the episode. The 'I didn't know that' moment.",

  "perfect_for": "Specific audience targeting. E.g., 'Founders raising capital' instead of 'Business people'.",

  "tags": ["tag1", "tag2", "tag3"]
}

RULES:
1. **Language**: CRITICAL - Write ALL content in the SAME LANGUAGE as the transcript.
2. **No Passive Voice**: Avoid "In this episode it is discussed...". Say "The host argues that...".
3. **Curiosity Gap**: The headline and brief should create curiosity.
4. **Specifics over Generalities**: Use specific numbers or names if available in the text.
5. **Speaker Names**: The transcript may use generic labels like "Speaker 1", "Speaker 2". IGNORE these labels. Instead, identify who each speaker is from conversational context — introductions, name mentions, how they address each other, and their role (host vs guest). Use real names in your output whenever identifiable.
`;

// DEEP summary prompt - returns DeepSummaryContent JSON
const DEEP_PROMPT = `You are an expert Ghostwriter and Analyst with a PhD in the subject matter of the transcript.
Your goal is to write a comprehensive "Executive Briefing" that completely substitutes the need to listen to the episode.

Return ONLY a JSON object with this EXACT structure. Every field is MANDATORY — do NOT omit any field.

{
  "comprehensive_overview": "A detailed, multi-paragraph essay (600-900 words, at least 4 paragraphs). MANDATORY: wrap exactly 3-5 of the most important sentences in <<double angle brackets>>. Example: The central claim is that <<quantum computing will make current encryption obsolete within 5 years>>, which forces a rethink of... The first paragraph must open with the central claim and stakes — never start with 'In this episode...'. Cover the full breadth of the discussion: arguments, counter-arguments, evidence cited, expert opinions, and practical implications.",

  "core_concepts": [
    {
      "concept": "Concept name",
      "explanation": "What it is + why it matters in this episode + what it changes for the listener (3-4 sentences).",
      "quote_reference": "A supporting quote from the episode (optional, omit key if none)"
    }
  ],

  "chronological_breakdown": [
    {
      "timestamp": "05:45",
      "timestamp_seconds": 345,
      "title": "Short chapter title (3-8 words)",
      "hook": "One-line curiosity-driven promise of what this section reveals (MANDATORY — never omit)",
      "content": "Detailed paragraph (90-160 words) covering what was said. Include speaker names and specific examples."
    }
  ],

  "contrarian_views": [
    "A 2-3 sentence view. State the contrarian claim, then pick ONE key term the audience may not know (or that is critical to understanding) and explain it. Example: 'Central banks flooding the market with liquidity may actually *increase* inequality. **Cantillon Effect** — the idea that newly printed money benefits those closest to the source first — means asset holders gain while wage earners fall behind.'"
  ],

  "actionable_takeaways": [
    {
      "text": "Verb-first, specific task the listener can do (e.g. 'Set up OpenTelemetry tracing for your services')",
      "category": "tool",
      "priority": "high",
      "resources": [
        { "name": "OpenTelemetry", "type": "tool", "context": "Monitoring framework discussed as the backbone for distributed tracing" }
      ]
    }
  ]
}

HARD RULES (violations = invalid output):

1. **Language**: ALL content MUST be in the SAME LANGUAGE as the transcript. Hebrew transcript → Hebrew output. No exceptions. No mixing languages within a field.

2. **<<Highlights>> are MANDATORY**: comprehensive_overview MUST contain exactly 3-5 sentences wrapped in <<double angle brackets>>. If your output has zero << >> markers, it is INVALID. Count them before responding.

3. **hook is MANDATORY**: Every item in chronological_breakdown MUST have a non-empty "hook" field. A hook is a one-line promise/insight that makes the reader want to read the section (e.g., "Why the CEO thinks remote work is dead"). If you omit hook from any chapter, the output is INVALID.

4. **Action items MUST be objects with resources**: Every item in actionable_takeaways MUST be a JSON object with "text", "category", "priority", and "resources" fields. NEVER return a plain string. The "text" field must start with a verb. Categories: tool/repo/concept/strategy/resource/habit. Priority: high/medium/low. The "resources" array MUST contain at least 1 resource per action item — link the action to a concrete tool, book, website, person, repo, or concept mentioned in the episode. If a specific resource wasn't named, infer the most relevant one (e.g., an action about "start journaling" → resource: {"name": "Morning Pages", "type": "book", "context": "Journaling technique from The Artist's Way"}). An empty resources array is INVALID.

5. **Contrarian views**: 4-8 views. Each view MUST be 2-3 sentences: state the contrarian claim, then bold ONE key term (**Term**) and interpret it for the reader. Every view must contain exactly one **bolded term** with an explanation.

6. **Core concepts**: 4-8 concept cards. Each explanation should cover what it is, why it matters here, and what it implies for the listener.

7. **Timestamps**: The transcript may include [MM:SS] timestamps. Set "timestamp" to the EXACT [MM:SS] where the topic begins, and "timestamp_seconds" to total seconds. If no timestamps exist, use "00:00" and 0.

8. **Tone**: Professional, analytical, engaging. Write directly: "Israel's geopolitical situation is shifting because..." — never "The speakers discussed..."

9. **No fluff**: Never start with "In this interesting/fascinating episode...". Dive straight into the content.

10. **Speaker Names**: The transcript may use generic labels like "Speaker 1", "Speaker 2". IGNORE these labels — they carry no meaning. Instead, identify who each speaker is from conversational context: introductions ("I'm Joe Rogan"), name mentions ("So Rachel, what do you think?"), how guests are addressed, and their role (host vs guest). ALWAYS use real names in your output (chronological_breakdown content, quotes, etc.). Never write "Speaker 1 said..." — write "Joe Rogan said..." or "the host argued...".

SELF-CHECK before responding:
- Does comprehensive_overview contain 3-5 << >> markers? If not → fix it.
- Does every chronological_breakdown item have a non-empty "hook"? If not → fix it.
- Is every actionable_takeaway an object with text/category/priority/resources where resources has ≥1 item? If not → fix it.
- Is ALL text in the transcript's language? If not → fix it.
- Does any text say "Speaker 1" or "Speaker 2"? If so → replace with real names from context, or "the host"/"the guest" if unidentifiable.
`;


/**
 * Fetch transcript text from a URL (supports SRT, VTT, plain text formats)
 * This is the FREE option - no Deepgram costs!
 */
async function fetchTranscriptFromUrl(transcriptUrl: string): Promise<string | null> {
  logWithTime('Attempting to fetch transcript from RSS URL (FREE)', { url: transcriptUrl.substring(0, 100) });
  
  try {
    const response = await fetch(transcriptUrl, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
      },
      signal: AbortSignal.timeout(30000), // 30 second timeout
    });

    if (!response.ok) {
      logWithTime('Transcript fetch failed', { status: response.status, statusText: response.statusText });
      return null;
    }

    const contentType = response.headers.get('content-type') || '';
    const text = await response.text();

    if (!text || text.trim().length < 50) {
      logWithTime('Transcript too short or empty', { length: text?.length });
      return null;
    }

    // Parse based on content type or file extension
    let parsed: string;
    if (contentType.includes('srt') || transcriptUrl.toLowerCase().endsWith('.srt')) {
      parsed = parseSrtToText(text);
    } else if (contentType.includes('vtt') || transcriptUrl.toLowerCase().endsWith('.vtt')) {
      parsed = parseVttToText(text);
    } else if (contentType.includes('json') || transcriptUrl.toLowerCase().endsWith('.json')) {
      parsed = parseJsonTranscript(text);
    } else {
      // Assume plain text - just clean it up
      parsed = text.trim();
    }

    logWithTime('Transcript fetched and parsed successfully (FREE)', { 
      originalLength: text.length, 
      parsedLength: parsed.length,
      format: contentType || 'unknown'
    });

    return parsed;
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error);
    logWithTime('Transcript fetch error', { error: errorMsg });
    return null;
  }
}

/**
 * Parse SRT format to plain text
 * SRT format: sequential number, timestamp line, text lines, blank line
 */
function parseSrtToText(srt: string): string {
  const lines = srt.split('\n');
  const textLines: string[] = [];

  for (const line of lines) {
    const trimmed = line.trim();

    // Skip sequence numbers (just digits)
    if (SRT_SEQUENCE_RE.test(trimmed)) {
      continue;
    }

    // Skip timestamp lines (00:00:00,000 --> 00:00:00,000)
    if (SRT_TIMESTAMP_RE.test(trimmed)) {
      continue;
    }

    // Skip empty lines
    if (trimmed === '') {
      continue;
    }

    // This is actual text content
    textLines.push(trimmed);
  }

  return textLines.join(' ').replace(MULTI_SPACE_RE, ' ').trim();
}

/**
 * Parse VTT format to plain text
 * VTT format similar to SRT but with WEBVTT header
 */
function parseVttToText(vtt: string): string {
  const lines = vtt.split('\n');
  const textLines: string[] = [];

  for (const line of lines) {
    const trimmed = line.trim();

    // Skip WEBVTT header and metadata
    if (trimmed.startsWith('WEBVTT') || trimmed.startsWith('NOTE') || trimmed.startsWith('STYLE')) {
      continue;
    }

    // Skip cue identifiers (if present)
    if (VTT_CUE_ID_RE.test(trimmed) && !trimmed.includes(' ')) {
      continue;
    }

    // Skip timestamp lines (00:00:00.000 --> 00:00:00.000)
    if (VTT_TIMESTAMP_RE.test(trimmed)) {
      continue;
    }

    // Skip empty lines
    if (trimmed === '') {
      continue;
    }

    // Strip VTT tags like <v Speaker Name>, <c>, </c>, etc.
    const cleanedLine = trimmed
      .replace(VTT_VOICE_TAG_RE, '')   // Voice tags
      .replace(VTT_VOICE_CLOSE_RE, '')
      .replace(VTT_CLASS_TAG_RE, '')   // Class tags
      .replace(VTT_CLASS_CLOSE_RE, '')
      .replace(VTT_ANY_TAG_RE, '')     // Any other tags
      .trim();

    if (cleanedLine) {
      textLines.push(cleanedLine);
    }
  }

  return textLines.join(' ').replace(MULTI_SPACE_RE, ' ').trim();
}

/**
 * Parse JSON transcript formats (various schemas)
 */
function parseJsonTranscript(json: string): string {
  try {
    const data = JSON.parse(json);
    
    // Handle array of segments
    if (Array.isArray(data)) {
      return data
        .map((item: unknown) => {
          if (typeof item === 'string') return item;
          if (typeof item === 'object' && item !== null) {
            const obj = item as Record<string, unknown>;
            return obj.text || obj.transcript || obj.content || '';
          }
          return '';
        })
        .filter(Boolean)
        .join(' ');
    }

    // Handle object with segments/utterances array
    if (data.segments || data.utterances || data.results) {
      const segments = data.segments || data.utterances || data.results;
      if (Array.isArray(segments)) {
        return segments
          .map((s: unknown) => {
            if (typeof s === 'object' && s !== null) {
              const obj = s as Record<string, unknown>;
              return obj.text || obj.transcript || '';
            }
            return '';
          })
          .filter(Boolean)
          .join(' ');
      }
    }

    // Handle object with direct text/transcript field
    if (data.text) return String(data.text);
    if (data.transcript) return String(data.transcript);

    // Fallback - stringify and clean
    return JSON.stringify(data);
  } catch {
    return json; // Return as-is if not valid JSON
  }
}

export async function ensureTranscript(
  episodeId: string,
  audioUrl: string,
  language = 'en',
  transcriptUrl?: string,  // Optional RSS transcript URL (FREE option!)
  metadata?: { podcastTitle: string; episodeTitle: string }  // For Apple Podcasts lookup
): Promise<{
  status: TranscriptStatus;
  text?: string;
  transcript?: DiarizedTranscript;
  pendingSpeakerIdentification?: boolean;
  error?: string;
}> {
  const startTime = Date.now();
  logWithTime('ensureTranscript started', { 
    episodeId, 
    audioUrl: audioUrl.substring(0, 80) + '...', 
    language,
    hasTranscriptUrl: !!transcriptUrl 
  });

  // Check if transcript exists
  logWithTime('Checking for existing transcript...');
  const dbCheckStart = Date.now();
  const { data: existing } = await supabase
    .from('transcripts')
    .select('id, episode_id, status, language, full_text, diarized_json, error_message, created_at, updated_at')
    .eq('episode_id', episodeId)
    .eq('language', language)
    .single();
  logWithTime('DB check completed', { durationMs: Date.now() - dbCheckStart, found: !!existing, status: existing?.status });

  if (existing) {
    if (existing.status === 'ready' && existing.full_text) {
      logWithTime('Returning cached transcript', { textLength: existing.full_text.length });
      // Also return diarized_json if available
      const diarizedTranscript = existing.diarized_json as DiarizedTranscript | null;
      return {
        status: 'ready',
        text: existing.full_text,
        transcript: diarizedTranscript || undefined
      };
    }
    if (existing.status === 'failed') {
      logWithTime('Previous transcript failed, will retry', { error: existing.error_message });
      // Don't return early - allow retry by continuing to transcription
    }
    if (existing.status === 'queued' || existing.status === 'transcribing') {
      logWithTime('Transcript already in progress', { status: existing.status });
      return { status: existing.status };
    }
  }

  // Create or update transcript record directly as transcribing (Fix 1: single DB write)
  logWithTime('Creating transcript record (transcribing)...');
  const { error: upsertError } = await supabase
    .from('transcripts')
    .upsert({
      episode_id: episodeId,
      language,
      status: 'transcribing',
      updated_at: new Date().toISOString()
    }, { onConflict: 'episode_id,language' });

  if (upsertError) {
    logWithTime('DB upsert error', { error: upsertError });
    return { status: 'failed', error: 'Database error' };
  }

  try {
    let transcriptText: string | null = null;
    let provider = 'deepgram';
    let diarizedTranscript: DiarizedTranscript | null = null;
    let pendingSpeakerIdentification = false;

    // ============================================
    // PRIORITY A+: Try Apple Podcasts transcript (FREE, instant!)
    // Apple has 125M+ episodes already transcribed
    // ============================================
    if (metadata?.podcastTitle && metadata?.episodeTitle) {
      logWithTime('PRIORITY A+: Attempting Apple Podcasts transcript (FREE, instant)...');
      try {
        const appleResult = await getAppleTranscript(metadata.podcastTitle, metadata.episodeTitle);
        if (appleResult) {
          provider = appleResult.provider;

          if (appleResult.diarized && appleResult.diarized.speakerCount > 1) {
            // Multi-speaker: defer identifySpeakers to run in parallel with summary generation
            diarizedTranscript = appleResult.diarized;
            diarizedTranscript.detectedLanguage = language;
            // Use generic "Speaker X" labels for now — real names will be identified
            // concurrently with summary generation in requestSummary
            transcriptText = formatTranscriptWithSpeakerNames(diarizedTranscript);
            pendingSpeakerIdentification = true;
            logWithTime('Apple diarized transcript ready (speaker names deferred for parallel execution)', {
              speakerCount: diarizedTranscript.speakerCount,
              utterances: diarizedTranscript.utterances.length,
            });
          } else if (appleResult.diarized) {
            // Single speaker: use diarized structure but skip speaker identification
            diarizedTranscript = appleResult.diarized;
            diarizedTranscript.detectedLanguage = language;
            transcriptText = appleResult.text;
          } else {
            // Fallback: plain text only (no diarization available)
            transcriptText = appleResult.text;
            diarizedTranscript = {
              utterances: [{ start: 0, end: 0, speaker: 0, text: transcriptText, confidence: 1.0 }],
              fullText: transcriptText,
              duration: 0,
              speakerCount: 1,
              detectedLanguage: language,
            };
          }

          logWithTime('SUCCESS: Got FREE transcript from Apple Podcasts!', {
            textLength: transcriptText.length,
            speakerCount: diarizedTranscript.speakerCount,
            hasSpeakerNames: !!diarizedTranscript.speakers?.length,
            saved: 'Deepgram/Voxtral API costs + ~5 min transcription time',
          });
        } else {
          logWithTime('PRIORITY A+ FAILED: Apple transcript not available, trying RSS...');
        }
      } catch (appleError) {
        const errorMsg = appleError instanceof Error ? appleError.message : String(appleError);
        logWithTime('PRIORITY A+ ERROR: Apple transcript fetch failed', { error: errorMsg });
      }
    }

    // ============================================
    // PRIORITY A: Try to fetch transcript from RSS URL (FREE!)
    // ============================================
    if (!transcriptText && transcriptUrl) {
      logWithTime('PRIORITY A: Attempting FREE transcript fetch from RSS URL...');
      transcriptText = await fetchTranscriptFromUrl(transcriptUrl);
      
      if (transcriptText && transcriptText.length > 100) {
        provider = 'rss-transcript';
        logWithTime('SUCCESS: Got FREE transcript from RSS!', { 
          textLength: transcriptText.length,
          saved: 'Deepgram API costs'
        });
        
        // Create a simple diarized transcript structure for RSS transcripts
        diarizedTranscript = {
          utterances: [{
            start: 0,
            end: 0,
            speaker: 0,
            text: transcriptText,
            confidence: 1.0
          }],
          fullText: transcriptText,
          duration: 0,
          speakerCount: 1,
          detectedLanguage: language
        };
      } else {
        logWithTime('PRIORITY A FAILED: RSS transcript fetch failed or too short, falling back to Deepgram');
      }
    }

    // ============================================
    // PRIORITY B1: Use Voxtral if language is supported (cheaper, built-in diarization)
    // ============================================
    if (!transcriptText && isVoxtralSupported(language)) {
      logWithTime('PRIORITY B1: Language supported by Voxtral, attempting Voxtral transcription...', { language });
      try {
        const voxtralStart = Date.now();
        diarizedTranscript = await transcribeWithVoxtral(audioUrl, language);
        provider = 'voxtral';
        logWithTime('Voxtral transcription succeeded', {
          durationMs: Date.now() - voxtralStart,
          utteranceCount: diarizedTranscript.utterances.length,
          speakerCount: diarizedTranscript.speakerCount,
        });

        // Check if transcription produced content
        if (!diarizedTranscript.fullText || diarizedTranscript.fullText.trim().length === 0) {
          logWithTime('Voxtral returned empty transcript, falling back to Deepgram');
          diarizedTranscript = null;
          provider = 'deepgram';
        } else {
          // Voxtral gives speaker labels (speaker_0, speaker_1) but NOT names.
          // Still call identifySpeakers() for name extraction from transcript context.
          logWithTime('Identifying speakers with LLM (Voxtral path)...');
          const speakers = await identifySpeakers(diarizedTranscript);
          diarizedTranscript.speakers = speakers;
          transcriptText = formatTranscriptWithSpeakerNames(diarizedTranscript);
        }
      } catch (err) {
        const errorMsg = err instanceof Error ? err.message : String(err);
        logWithTime('Voxtral transcription FAILED, falling back to Deepgram', { error: errorMsg });
        // Reset for Deepgram fallback
        diarizedTranscript = null;
        provider = 'deepgram';
      }
    } else if (!transcriptText) {
      logWithTime('PRIORITY B1: Language not supported by Voxtral, skipping to Deepgram', { language });
    }

    // ============================================
    // PRIORITY B2: Use Deepgram with explicit language (fallback)
    // ============================================
    if (!transcriptText) {
      logWithTime('PRIORITY B: Starting transcription via Deepgram with explicit language...');
      const transcribeStart = Date.now();
      
      // ALWAYS pass language explicitly to Deepgram - this avoids paying for language detection
      // and improves transcription accuracy
      logWithTime('Passing explicit language to Deepgram', { language });
      diarizedTranscript = await transcribeFromUrl(audioUrl, language);
      
      const formattedText = formatTranscriptWithTimestamps(diarizedTranscript);
      logWithTime('Deepgram transcription completed', {
        durationMs: Date.now() - transcribeStart,
        durationSec: ((Date.now() - transcribeStart) / 1000).toFixed(1),
        textLength: formattedText.length,
        utteranceCount: diarizedTranscript.utterances.length,
        speakerCount: diarizedTranscript.speakerCount,
        detectedLanguage: diarizedTranscript.detectedLanguage
      });

      // Check if transcription produced any content
      if (!diarizedTranscript.fullText || diarizedTranscript.fullText.trim().length === 0) {
        const errorMsg = 'Transcription returned empty - audio may be unsupported or corrupted';
        logWithTime('Transcription returned empty content', { 
          utteranceCount: diarizedTranscript.utterances.length,
          fullTextLength: diarizedTranscript.fullText?.length || 0
        });
        await supabase
          .from('transcripts')
          .update({ status: 'failed', error_message: errorMsg })
          .eq('episode_id', episodeId)
          .eq('language', language);
        return { status: 'failed', error: errorMsg };
      }

      // Identify speakers using LLM
      logWithTime('Identifying speakers with LLM...');
      const speakers = await identifySpeakers(diarizedTranscript);
      diarizedTranscript.speakers = speakers;

      // Re-format transcript with identified speaker names
      transcriptText = formatTranscriptWithSpeakerNames(diarizedTranscript);
    }

    // Save transcript to DB (language is known from RSS feed)
    logWithTime('Saving transcript to DB...', { provider, language });
    const saveStart = Date.now();
    await supabase
      .from('transcripts')
      .update({
        status: 'ready',
        full_text: transcriptText,
        diarized_json: diarizedTranscript,
        provider
      })
      .eq('episode_id', episodeId)
      .eq('language', language);
    logWithTime('Transcript saved', { durationMs: Date.now() - saveStart });

    logWithTime('ensureTranscript completed successfully', { 
      totalDurationMs: Date.now() - startTime,
      language,
      provider,
      wasFree: provider === 'rss-transcript' || provider === 'apple-podcasts'
    });
    return { status: 'ready', text: transcriptText, transcript: diarizedTranscript || undefined, pendingSpeakerIdentification };
  } catch (err) {
    const errorMsg = err instanceof Error ? err.message : 'Transcription failed';
    logWithTime('Transcription FAILED', { error: errorMsg, totalDurationMs: Date.now() - startTime });
    await supabase
      .from('transcripts')
      .update({ status: 'failed', error_message: errorMsg })
      .eq('episode_id', episodeId)
      .eq('language', language);

    return { status: 'failed', error: errorMsg };
  }
}

export async function generateSummaryForLevel(
  episodeId: string,
  level: SummaryLevel,
  transcriptText: string,
  language = 'en',
  diarizedTranscript?: DiarizedTranscript
): Promise<{ status: SummaryStatus; content?: QuickSummaryContent | DeepSummaryContent; error?: string }> {
  const startTime = Date.now();
  logWithTime('generateSummaryForLevel started', { episodeId, level, language, transcriptLength: transcriptText.length });

  // Update status to summarizing
  logWithTime('Updating status to summarizing...');
  await supabase
    .from('summaries')
    .update({ status: 'summarizing' })
    .eq('episode_id', episodeId)
    .eq('level', level)
    .eq('language', language);

  try {
    // Get the appropriate model based on level
    // Deep uses Pro (more capable), Quick uses Flash (faster, cheaper)
    const selectedModel = getModelForLevel(level);
    const modelName = level === 'deep' ? 'gemini-3.1-pro-preview' : 'gemini-3-flash-preview';
    const prompt = level === 'quick' ? QUICK_PROMPT : DEEP_PROMPT;

    // Use up to 150k characters of transcript (leaves room for prompt and response)
    const maxTranscriptChars = 150000;

    // For deep summaries, use diarized transcript with timestamps when available
    // This enables Gemini to extract real timestamps for chronological_breakdown
    let inputTranscript = transcriptText;
    if (level === 'deep' && diarizedTranscript?.utterances && diarizedTranscript.utterances.length > 0) {
      const timestampedTranscript = formatTranscriptWithTimestamps(diarizedTranscript);
      if (timestampedTranscript.length > 0) {
        inputTranscript = timestampedTranscript;
        logWithTime('Using diarized transcript with timestamps for deep summary', {
          utteranceCount: diarizedTranscript.utterances.length,
          timestampedLength: timestampedTranscript.length
        });
      }
    }

    const truncatedTranscript = inputTranscript.length > maxTranscriptChars
      ? inputTranscript.substring(0, maxTranscriptChars) + '\n\n[... transcript truncated for length ...]'
      : inputTranscript;

    const inputLength = (prompt + truncatedTranscript).length;
    logWithTime(`Generating ${level.toUpperCase()} Summary via Gemini...`, {
      model: modelName,
      level,
      inputLength,
      transcriptTruncated: inputTranscript.length > maxTranscriptChars,
      usedDiarizedTranscript: level === 'deep' && inputTranscript !== transcriptText
    });

    const apiStart = Date.now();
    const fullPrompt = SYSTEM_MESSAGE + "\n\n" + prompt + truncatedTranscript;
    
    const result = await selectedModel.generateContent(fullPrompt);
    const response = result.response;
    const text = response.text();
    
    logWithTime(`Gemini API completed for ${level.toUpperCase()} Summary`, {
      model: modelName,
      durationMs: Date.now() - apiStart,
      durationSec: ((Date.now() - apiStart) / 1000).toFixed(1)
    });

    logWithTime('Parsing JSON response...', { responseLength: text.length });

    // Try to extract JSON from the response (in case model added text before/after)
    let jsonText = text.trim();

    // If response doesn't start with {, try to find JSON in the response
    if (!jsonText.startsWith('{')) {
      const jsonMatch = jsonText.match(/\{[\s\S]*\}/);
      if (jsonMatch) {
        logWithTime('Extracted JSON from wrapped response');
        jsonText = jsonMatch[0];
      } else {
        logWithTime('No JSON found in response', { responsePreview: text.substring(0, 500) });
        throw new Error('No JSON object found in response');
      }
    }

    // Try to parse JSON, with repair on failure
    let content: QuickSummaryContent | DeepSummaryContent;
    try {
      content = JSON.parse(jsonText);
    } catch (parseError) {
      const parseMsg = parseError instanceof Error ? parseError.message : String(parseError);
      // Log the area around the failure for debugging
      const posMatch = parseMsg.match(/position (\d+)/);
      const failPos = posMatch ? parseInt(posMatch[1]) : -1;
      logWithTime('Initial JSON parse failed, attempting repair...', {
        error: parseMsg,
        failContext: failPos >= 0 ? jsonText.substring(Math.max(0, failPos - 60), failPos + 60) : undefined
      });
      const repaired = repairJsonString(jsonText);
      try {
        content = JSON.parse(repaired);
        logWithTime('JSON repair succeeded');
      } catch (repairError) {
        const repairMsg = repairError instanceof Error ? repairError.message : String(repairError);
        logWithTime('JSON repair also failed', { error: repairMsg });
        throw new Error(`Invalid JSON from Gemini (repair failed): ${parseMsg}`);
      }
    }

    logWithTime('Saving summary to DB...');
    const saveStart = Date.now();
    await supabase
      .from('summaries')
      .update({
        status: 'ready',
        content_json: content,
        error_message: null
      })
      .eq('episode_id', episodeId)
      .eq('level', level)
      .eq('language', language);
    logWithTime('Summary saved', { durationMs: Date.now() - saveStart });

    // Invalidate stale status caches so the insights page picks up the new summary
    try {
      const { deleteCached, CacheKeys } = await import('@/lib/cache');
      await Promise.all([
        deleteCached(CacheKeys.insightsStatus(episodeId, language)),
        deleteCached(CacheKeys.summaryStatus(episodeId, language)),
      ]);
      logWithTime('Invalidated status caches', { episodeId, language });
    } catch (cacheErr) {
      logWithTime('Cache invalidation failed (non-blocking)', { error: String(cacheErr) });
    }

    // Trigger pending notifications (non-blocking)
    try {
      await triggerPendingNotifications(episodeId);
    } catch (notifError) {
      logWithTime('Notification trigger failed (non-blocking)', { episodeId, error: String(notifError) });
    }

    logWithTime('generateSummaryForLevel completed successfully', {
      level,
      totalDurationMs: Date.now() - startTime,
      totalDurationSec: ((Date.now() - startTime) / 1000).toFixed(1)
    });
    return { status: 'ready', content };
  } catch (err) {
    const errorMsg = err instanceof Error ? err.message : 'Summary generation failed';
    logWithTime('Summary generation FAILED', { error: errorMsg, totalDurationMs: Date.now() - startTime });
    await supabase
      .from('summaries')
      .update({ status: 'failed', error_message: errorMsg })
      .eq('episode_id', episodeId)
      .eq('level', level)
      .eq('language', language);

    return { status: 'failed', error: errorMsg };
  }
}

/**
 * Quick check for existing summary — returns immediately without starting generation.
 * Used by the API route to return cached/in-progress results without blocking.
 */
export async function checkExistingSummary(
  episodeId: string,
  level: SummaryLevel,
  language = 'en'
): Promise<{ status: SummaryStatus; content?: QuickSummaryContent | DeepSummaryContent } | null> {
  const { data: existingSummaries } = await supabase
    .from('summaries')
    .select('id, status, content_json, updated_at')
    .eq('episode_id', episodeId)
    .eq('level', level)
    .eq('language', language);

  if (!existingSummaries || existingSummaries.length === 0) return null;

  // Find the best summary by status priority
  let best: any = null;
  let bestPriority = 0;
  for (const summary of existingSummaries) {
    const priority = SUMMARY_STATUS_PRIORITY[summary.status] || 0;
    if (priority > bestPriority) {
      bestPriority = priority;
      best = summary;
    }
  }

  if (!best) return null;

  if (best.status === 'ready' && best.content_json) {
    return { status: 'ready', content: best.content_json };
  }
  if (['queued', 'transcribing', 'summarizing'].includes(best.status)) {
    // Check for stale summaries (stuck > 30 min)
    const STALE_THRESHOLD_MS = 30 * 60 * 1000;
    const updatedAt = best.updated_at ? new Date(best.updated_at).getTime() : 0;
    if (Date.now() - updatedAt > STALE_THRESHOLD_MS) {
      return null; // Stale — let requestSummary retry
    }
    return { status: best.status as SummaryStatus };
  }
  // Failed or other — let requestSummary retry
  return null;
}

export async function requestSummary(
  episodeId: string,
  level: SummaryLevel,
  audioUrl: string,
  language = 'en',
  transcriptUrl?: string,
  metadata?: { podcastTitle: string; episodeTitle: string }
): Promise<{ status: SummaryStatus; content?: QuickSummaryContent | DeepSummaryContent }> {
  const startTime = Date.now();
  logWithTime('=== requestSummary STARTED ===', { episodeId, level, language, hasTranscriptUrl: !!transcriptUrl, hasMetadata: !!metadata });

  // Check existing summary
  logWithTime('Checking for existing summary...');
  const checkStart = Date.now();
  
  // Fetch ALL summaries for this episode/level/language (not .single() to handle duplicates)
  const { data: existingSummaries } = await supabase
    .from('summaries')
    .select('id, episode_id, level, status, language, content_json, error_message, created_at, updated_at')
    .eq('episode_id', episodeId)
    .eq('level', level)
    .eq('language', language);
  
  // Find the BEST summary (highest priority status)
  let existing: any = null;
  let bestPriority = 0;
  
  if (existingSummaries && existingSummaries.length > 0) {
    for (const summary of existingSummaries) {
      const priority = SUMMARY_STATUS_PRIORITY[summary.status] || 0;
      if (priority > bestPriority) {
        bestPriority = priority;
        existing = summary;
      }
    }
  }
  
  logWithTime('Existing summary check completed', { 
    durationMs: Date.now() - checkStart, 
    found: !!existing,
    totalFound: existingSummaries?.length || 0,
    status: existing?.status 
  });

  if (existing) {
    if (existing.status === 'ready' && existing.content_json) {
      logWithTime('Returning cached summary', { totalDurationMs: Date.now() - startTime });
      return { status: 'ready', content: existing.content_json };
    }
    if (['queued', 'transcribing', 'summarizing'].includes(existing.status)) {
      // Check for stale summaries stuck in processing for over 30 minutes
      const STALE_THRESHOLD_MS = 30 * 60 * 1000; // 30 minutes
      const updatedAt = existing.updated_at ? new Date(existing.updated_at).getTime() : 0;
      const isStale = Date.now() - updatedAt > STALE_THRESHOLD_MS;

      if (isStale) {
        logWithTime('Summary is STALE - resetting to retry', {
          status: existing.status,
          updatedAt: existing.updated_at,
          staleForMs: Date.now() - updatedAt
        });
        // Reset to failed so it gets retried below
        await supabase
          .from('summaries')
          .update({ status: 'failed', updated_at: new Date().toISOString() })
          .eq('id', existing.id);
      } else {
        logWithTime('Summary already in progress', { status: existing.status, totalDurationMs: Date.now() - startTime });
        return { status: existing.status as SummaryStatus };
      }
    }
    logWithTime('Summary exists but needs retry', { status: existing.status });
    // If failed or not_ready, we'll try again
  }

  // Create summary record directly as transcribing (Fix 1: single DB write)
  logWithTime('Creating summary record (transcribing)...');
  await supabase
    .from('summaries')
    .upsert({
      episode_id: episodeId,
      level,
      language,
      status: 'transcribing',
      updated_at: new Date().toISOString()
    }, { onConflict: 'episode_id,level,language' });

  // Ensure transcript exists (this is blocking for now, could be async)
  logWithTime('Calling ensureTranscript...', { hasTranscriptUrl: !!transcriptUrl, hasMetadata: !!metadata });
  const transcriptResult = await ensureTranscript(episodeId, audioUrl, language, transcriptUrl, metadata);
  logWithTime('ensureTranscript returned', { status: transcriptResult.status, hasText: !!transcriptResult.text, hasTranscript: !!transcriptResult.transcript, error: transcriptResult.error });

  if (transcriptResult.status !== 'ready' || !transcriptResult.text) {
    // Update summary status to match transcript status
    const summaryStatus: SummaryStatus = transcriptResult.status === 'failed' ? 'failed' : 'transcribing';
    logWithTime('Transcript not ready, updating summary status', { summaryStatus });
    await supabase
      .from('summaries')
      .update({
        status: summaryStatus,
        error_message: transcriptResult.error || null
      })
      .eq('episode_id', episodeId)
      .eq('level', level)
      .eq('language', language);

    logWithTime('=== requestSummary ENDED (transcript not ready) ===', { totalDurationMs: Date.now() - startTime });
    return { status: summaryStatus };
  }

  // Generate the summary (language is known from RSS feed)
  // If speaker identification is pending (Apple multi-speaker), run it in parallel
  // with summary generation to save ~20s
  if (transcriptResult.pendingSpeakerIdentification && transcriptResult.transcript) {
    logWithTime('Running identifySpeakers in PARALLEL with generateSummaryForLevel...', { language });
    const [result, speakers] = await Promise.all([
      generateSummaryForLevel(episodeId, level, transcriptResult.text, language, transcriptResult.transcript),
      identifySpeakers(transcriptResult.transcript),
    ]);

    // Update transcript in DB with real speaker names (non-blocking for the response)
    if (speakers.length > 0) {
      transcriptResult.transcript.speakers = speakers;
      const namedTranscript = formatTranscriptWithSpeakerNames(transcriptResult.transcript);
      logWithTime('Updating transcript with identified speaker names...', {
        speakers: speakers.map(s => ({ id: s.id, name: s.name, role: s.role })),
      });
      supabase
        .from('transcripts')
        .update({
          full_text: namedTranscript,
          diarized_json: transcriptResult.transcript,
        })
        .eq('episode_id', episodeId)
        .eq('language', transcriptResult.transcript.detectedLanguage || 'en')
        .then(({ error }) => {
          if (error) logWithTime('Failed to update transcript with speaker names', { error });
          else logWithTime('Transcript updated with speaker names');
        });
    }

    logWithTime('=== requestSummary ENDED ===', {
      status: result.status,
      language,
      parallelSpeakerIdentification: true,
      totalDurationMs: Date.now() - startTime,
      totalDurationSec: ((Date.now() - startTime) / 1000).toFixed(1),
    });
    return result;
  }

  logWithTime('Calling generateSummaryForLevel...', { language });
  const result = await generateSummaryForLevel(
    episodeId,
    level,
    transcriptResult.text,
    language,
    transcriptResult.transcript
  );
  logWithTime('=== requestSummary ENDED ===', {
    status: result.status,
    language,
    totalDurationMs: Date.now() - startTime,
    totalDurationSec: ((Date.now() - startTime) / 1000).toFixed(1)
  });
  return result;
}

export async function getSummariesStatus(episodeId: string, language = 'en') {
  // Check Redis cache for terminal states
  const { getCached, setCached, CacheKeys, CacheTTL } = await import('@/lib/cache');
  const cacheKey = CacheKeys.summaryStatus(episodeId, language);
  const cached = await getCached<any>(cacheKey);
  if (cached) return cached;

  // Find transcript in ANY language for this episode (handles auto-detected languages)
  const { data: transcripts } = await supabase
    .from('transcripts')
    .select('status, language')
    .eq('episode_id', episodeId)
    .order('created_at', { ascending: false });

  const transcript = transcripts?.[0] || null;
  const actualLanguage = transcript?.language || language;

  // Fetch summaries with the actual language
  const { data: summaries } = await supabase
    .from('summaries')
    .select('id, episode_id, level, status, language, content_json, updated_at')
    .eq('episode_id', episodeId)
    .eq('language', actualLanguage)
    .in('level', ['quick', 'deep']);

  const quick = summaries?.find(s => s.level === 'quick');
  const deep = summaries?.find(s => s.level === 'deep');

  const result = {
    episodeId,
    detected_language: actualLanguage,
    transcript: transcript ? { status: transcript.status, language: transcript.language } : null,
    summaries: {
      quick: quick ? {
        status: quick.status,
        content: quick.status === 'ready' ? quick.content_json : undefined,
        updatedAt: quick.updated_at
      } : null,
      deep: deep ? {
        status: deep.status,
        content: deep.status === 'ready' ? deep.content_json : undefined,
        updatedAt: deep.updated_at
      } : null
    }
  };

  // Cache based on state
  const hasAnySummary = !!(quick || deep);
  const quickTerminal = !quick || quick.status === 'ready' || quick.status === 'failed';
  const deepTerminal = !deep || deep.status === 'ready' || deep.status === 'failed';

  if (hasAnySummary && quickTerminal && deepTerminal) {
    // Terminal states: cache for a long time
    await setCached(cacheKey, result, CacheTTL.STATUS_TERMINAL);
  } else if (hasAnySummary) {
    // In-progress states: cache for 10 seconds to reduce DB polling pressure
    await setCached(cacheKey, result, 10);
  }

  return result;
}
