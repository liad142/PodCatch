import { createClient } from '@deepgram/sdk';
import type {
  DeepgramResponse,
  DiarizedTranscript,
  Utterance,
} from '@/types/deepgram';

const deepgram = createClient(process.env.DEEPGRAM_API_KEY!);

const isDev = process.env.NODE_ENV === 'development';

async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = 3,
  baseDelayMs: number = 1000
): Promise<T> {
  let lastError: Error | null = null;
  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error: any) {
      lastError = error;
      // Don't retry on client errors (4xx)
      if (error?.status >= 400 && error?.status < 500) throw error;
      if (attempt < maxRetries) {
        const delay = baseDelayMs * Math.pow(2, attempt);
        console.log(`[Deepgram] Attempt ${attempt + 1} failed, retrying in ${delay}ms...`);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }
  throw lastError;
}

import { createLogger } from "@/lib/logger";
const logWithTime = createLogger('DEEPGRAM');

// Audio file extensions that indicate direct URLs (no redirects needed)
const DIRECT_AUDIO_EXTENSIONS = ['.mp3', '.m4a', '.wav', '.ogg', '.flac', '.aac', '.opus'];

/**
 * Check if URL appears to be a direct audio file (no tracking redirect needed)
 */
function isDirectAudioUrl(url: string): boolean {
  const urlLower = url.toLowerCase();
  // Check file extension
  if (DIRECT_AUDIO_EXTENSIONS.some(ext => urlLower.includes(ext))) {
    // Make sure it's not a tracking URL that contains the extension in the path
    const urlObj = new URL(url);
    const pathname = urlObj.pathname.toLowerCase();
    if (DIRECT_AUDIO_EXTENSIONS.some(ext => pathname.endsWith(ext))) {
      return true;
    }
  }
  return false;
}

/**
 * Check if a Deepgram error is a remote content error (host blocked the request)
 */
function isRemoteContentError(error: unknown): boolean {
  const msg = error instanceof Error ? error.message : String(error);
  return msg.includes('REMOTE_CONTENT_ERROR') || msg.includes('403 Forbidden');
}

/**
 * Follow redirects to get the final audio URL
 * Many podcast URLs go through tracking services that Deepgram can't fetch
 * Optimized: skips redirect following for direct audio URLs and uses timeouts
 */
async function resolveAudioUrl(url: string, maxRedirects = 5): Promise<string> {
  // Skip redirect following for direct audio URLs (common case)
  if (isDirectAudioUrl(url)) {
    logWithTime('URL appears to be direct audio, skipping redirect resolution');
    return url;
  }

  let currentUrl = url;
  let redirectCount = 0;
  const REDIRECT_TIMEOUT = 3000; // 3 second timeout per redirect

  while (redirectCount < maxRedirects) {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), REDIRECT_TIMEOUT);
    try {
      const response = await fetch(currentUrl, {
        method: 'HEAD',
        redirect: 'manual', // Don't auto-follow, we want to track
        headers: {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        },
        signal: controller.signal,
      });

      // Check if it's a redirect (3xx status)
      if (response.status >= 300 && response.status < 400) {
        const location = response.headers.get('location');
        if (!location) {
          logWithTime('Redirect without location header, using current URL');
          break;
        }

        // Handle relative URLs
        currentUrl = location.startsWith('http')
          ? location
          : new URL(location, currentUrl).toString();

        redirectCount++;
        logWithTime(`Following redirect ${redirectCount}`, { to: currentUrl.substring(0, 80) + '...' });

        // If we've resolved to a direct audio URL, stop here
        if (isDirectAudioUrl(currentUrl)) {
          logWithTime('Resolved to direct audio URL, stopping redirect chain');
          break;
        }
      } else {
        // Not a redirect, we're done
        break;
      }
    } catch (error) {
      // On timeout or error, use current URL and continue
      if (error instanceof Error && error.name === 'AbortError') {
        logWithTime('Redirect request timed out, using current URL');
      } else {
        logWithTime('Error resolving URL, using current', { error: String(error) });
      }
      break;
    } finally {
      clearTimeout(timeoutId);
    }
  }

  if (redirectCount > 0) {
    logWithTime(`Resolved URL after ${redirectCount} redirects`, {
      original: url.substring(0, 60) + '...',
      resolved: currentUrl.substring(0, 60) + '...'
    });
  }

  return currentUrl;
}

/**
 * Force-resolve a URL by always following redirects, even for .mp3 URLs.
 * Used as a fallback when the initial attempt fails.
 */
async function forceResolveAudioUrl(url: string, maxRedirects = 10): Promise<string> {
  let currentUrl = url;
  let redirectCount = 0;
  const REDIRECT_TIMEOUT = 5000;

  while (redirectCount < maxRedirects) {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), REDIRECT_TIMEOUT);
    try {
      const response = await fetch(currentUrl, {
        method: 'HEAD',
        redirect: 'manual',
        headers: {
          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
          'Accept': 'audio/mpeg, audio/*, */*',
        },
        signal: controller.signal,
      });

      if (response.status >= 300 && response.status < 400) {
        const location = response.headers.get('location');
        if (!location) break;

        currentUrl = location.startsWith('http')
          ? location
          : new URL(location, currentUrl).toString();
        redirectCount++;
        logWithTime(`Force-resolve redirect ${redirectCount}`, { to: currentUrl.substring(0, 80) + '...' });
      } else {
        break;
      }
    } catch {
      break;
    } finally {
      clearTimeout(timeoutId);
    }
  }

  return currentUrl;
}

/**
 * Download audio from URL into a Buffer (our server fetches it, not Deepgram).
 * This bypasses any host-level blocking of Deepgram's IPs.
 */
async function downloadAudioBuffer(url: string): Promise<Buffer> {
  const DOWNLOAD_TIMEOUT = 120_000; // 2 minutes for large files
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), DOWNLOAD_TIMEOUT);

  try {
    const response = await fetch(url, {
      headers: {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'audio/mpeg, audio/*, */*',
      },
      redirect: 'follow',
      signal: controller.signal,
    });

    if (!response.ok) {
      throw new Error(`Download failed: ${response.status} ${response.statusText}`);
    }

    const arrayBuffer = await response.arrayBuffer();
    return Buffer.from(arrayBuffer);
  } finally {
    clearTimeout(timeoutId);
  }
}

/**
 * Build Deepgram transcription config
 */
function buildDeepgramConfig(language?: string): Record<string, unknown> {
  const config: Record<string, unknown> = {
    model: 'whisper-large',
    diarize: true,
    utterances: true,
    smart_format: true,
    punctuate: true,
    detect_language: false,
  };

  config.language = language || 'en';
  return config;
}

function parseDeepgramResponse(response: DeepgramResponse): DiarizedTranscript {
  const utterances: Utterance[] = [];
  let fullText = '';

  // Try to get utterances first (preferred - has speaker diarization)
  if (response.results.utterances && response.results.utterances.length > 0) {
    for (const utt of response.results.utterances) {
      utterances.push({
        start: utt.start,
        end: utt.end,
        speaker: utt.speaker,
        text: utt.transcript,
        confidence: utt.confidence,
      });
    }
    fullText = utterances.map(u => u.text).join(' ');
  }
  // Fallback: Get transcript from channels if no utterances
  else if (response.results.channels?.[0]?.alternatives?.[0]) {
    const channel = response.results.channels[0];
    const alternative = channel.alternatives[0];
    fullText = alternative.transcript || '';

    // Create a single utterance from the full transcript if we have words with timing
    if (alternative.words && alternative.words.length > 0) {
      // Group words by speaker for diarization
      let currentSpeaker = 0;
      let currentStart = alternative.words[0].start;
      let currentText: string[] = [];

      for (const word of alternative.words) {
        const wordSpeaker = word.speaker ?? 0;

        if (wordSpeaker !== currentSpeaker && currentText.length > 0) {
          // Save current utterance
          utterances.push({
            start: currentStart,
            end: word.start,
            speaker: currentSpeaker,
            text: currentText.join(' '),
            confidence: 0.9,
          });
          currentText = [];
          currentStart = word.start;
          currentSpeaker = wordSpeaker;
        }
        currentText.push(word.punctuated_word || word.word);
      }

      // Save last utterance
      if (currentText.length > 0) {
        const lastWord = alternative.words[alternative.words.length - 1];
        utterances.push({
          start: currentStart,
          end: lastWord.end,
          speaker: currentSpeaker,
          text: currentText.join(' '),
          confidence: 0.9,
        });
      }
    } else if (fullText) {
      // No word-level timing, create single utterance
      utterances.push({
        start: 0,
        end: response.metadata.duration,
        speaker: 0,
        text: fullText,
        confidence: 0.9,
      });
    }
  }

  // Count unique speakers
  const speakerSet = new Set(utterances.map(u => u.speaker));

  // Get detected language from first channel
  const detectedLanguage = (response.results.channels?.[0] as { detected_language?: string })?.detected_language;

  logWithTime('parseDeepgramResponse result', {
    hasUtterances: response.results.utterances?.length ?? 0,
    parsedUtterances: utterances.length,
    fullTextLength: fullText.length,
    detectedLanguage
  });

  return {
    utterances,
    fullText,
    duration: response.metadata.duration,
    speakerCount: speakerSet.size || 1,
    detectedLanguage,
  };
}

/**
 * Transcribe audio with a 3-step fallback chain:
 *  1. Pass URL directly to Deepgram (fast path, works for most podcasts)
 *  2. If REMOTE_CONTENT_ERROR: force-resolve redirects and retry with resolved URL
 *  3. If still fails: download audio ourselves and send raw bytes to Deepgram
 */
export async function transcribeFromUrl(
  audioUrl: string,
  language?: string // Optional: 'en', 'he', etc. If not provided, Deepgram auto-detects
): Promise<DiarizedTranscript> {
  logWithTime('transcribeFromUrl started', {
    audioUrl: audioUrl.substring(0, 100) + '...',
    language: language || 'auto-detect'
  });

  const startTime = Date.now();
  const config = buildDeepgramConfig(language);

  // ── Step 1: Try with resolved URL (optimized path) ──
  try {
    logWithTime('Step 1: Resolving audio URL (following redirects)...');
    const resolvedUrl = await resolveAudioUrl(audioUrl);

    logWithTime('Sending to Deepgram API...', config);

    const { result, error } = await withRetry(() =>
      deepgram.listen.prerecorded.transcribeUrl(
        { url: resolvedUrl },
        config
      )
    );

    if (error) {
      throw new Error(`Deepgram API error: ${JSON.stringify(error)}`);
    }

    const duration = Date.now() - startTime;
    logWithTime('Step 1 succeeded', {
      durationMs: duration,
      audioDuration: result.metadata?.duration,
      utteranceCount: result.results?.utterances?.length || 0,
    });

    return parseDeepgramResponse(result as DeepgramResponse);
  } catch (step1Error) {
    if (!isRemoteContentError(step1Error)) {
      // Not a remote content error — don't bother with fallbacks
      const duration = Date.now() - startTime;
      const errorMsg = step1Error instanceof Error ? step1Error.message : String(step1Error);
      logWithTime('Step 1 FAILED (non-recoverable)', { durationMs: duration, error: errorMsg });
      throw new (class extends Error { name = 'TranscriptionError'; })(
        `Deepgram transcription failed: ${errorMsg}`
      );
    }

    logWithTime('Step 1 FAILED with REMOTE_CONTENT_ERROR, trying Step 2 (force-resolve redirects)...');
  }

  // ── Step 2: Force-resolve all redirects (even for .mp3 URLs) and retry ──
  try {
    const resolvedUrl = await forceResolveAudioUrl(audioUrl);
    logWithTime('Step 2: Force-resolved URL', { resolved: resolvedUrl.substring(0, 100) + '...' });

    // Only try if we got a different URL
    if (resolvedUrl !== audioUrl) {
      const { result, error } = await withRetry(() =>
        deepgram.listen.prerecorded.transcribeUrl(
          { url: resolvedUrl },
          config
        )
      );

      if (error) {
        throw new Error(`Deepgram API error: ${JSON.stringify(error)}`);
      }

      const duration = Date.now() - startTime;
      logWithTime('Step 2 succeeded', {
        durationMs: duration,
        audioDuration: result.metadata?.duration,
        utteranceCount: result.results?.utterances?.length || 0,
      });

      return parseDeepgramResponse(result as DeepgramResponse);
    } else {
      logWithTime('Step 2: URL unchanged after force-resolve, skipping to Step 3');
    }
  } catch (step2Error) {
    logWithTime('Step 2 FAILED, trying Step 3 (download audio ourselves)...', {
      error: step2Error instanceof Error ? step2Error.message : String(step2Error),
    });
  }

  // ── Step 3: Download audio ourselves and send raw bytes to Deepgram ──
  try {
    logWithTime('Step 3: Downloading audio to buffer...');
    const downloadStart = Date.now();
    const audioBuffer = await downloadAudioBuffer(audioUrl);
    logWithTime('Step 3: Audio downloaded', {
      durationMs: Date.now() - downloadStart,
      sizeBytes: audioBuffer.length,
      sizeMB: (audioBuffer.length / (1024 * 1024)).toFixed(1),
    });

    logWithTime('Step 3: Sending audio buffer to Deepgram...');
    const { result, error } = await withRetry(() =>
      deepgram.listen.prerecorded.transcribeFile(
        audioBuffer,
        config
      )
    );

    if (error) {
      throw new Error(`Deepgram API error: ${JSON.stringify(error)}`);
    }

    const duration = Date.now() - startTime;
    logWithTime('Step 3 succeeded', {
      totalDurationMs: duration,
      audioDuration: result.metadata?.duration,
      utteranceCount: result.results?.utterances?.length || 0,
    });

    return parseDeepgramResponse(result as DeepgramResponse);
  } catch (step3Error) {
    const duration = Date.now() - startTime;
    const errorMsg = step3Error instanceof Error ? step3Error.message : String(step3Error);
    logWithTime('All 3 steps FAILED', { totalDurationMs: duration, error: errorMsg });

    throw new (class extends Error { name = 'TranscriptionError'; })(
      `Deepgram transcription failed after all fallbacks: ${errorMsg}`
    );
  }
}

// Helper to format transcript with timestamps for legacy compatibility
export function formatTranscriptWithTimestamps(transcript: DiarizedTranscript): string {
  return transcript.utterances
    .map(u => {
      const mins = Math.floor(u.start / 60);
      const secs = Math.floor(u.start % 60);
      const timestamp = `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
      return `[${timestamp}] [Speaker ${u.speaker}] ${u.text}`;
    })
    .join('\n');
}

// Format transcript with identified speaker names
// Merges consecutive utterances from the same speaker into paragraphs
export function formatTranscriptWithSpeakerNames(transcript: DiarizedTranscript): string {
  const speakerMap = new Map<number, string>();

  // Build speaker name map
  if (transcript.speakers) {
    for (const speaker of transcript.speakers) {
      speakerMap.set(speaker.id, speaker.name);
    }
  }

  if (transcript.utterances.length === 0) {
    return '';
  }

  // Merge consecutive utterances from the same speaker
  // Only split when: speaker changes OR there's a gap > 5 seconds
  const MAX_GAP_SECONDS = 5;
  const mergedBlocks: { speaker: number; start: number; texts: string[] }[] = [];

  let currentBlock: { speaker: number; start: number; texts: string[] } | null = null;
  let lastEnd = 0;

  for (const u of transcript.utterances) {
    const gap = u.start - lastEnd;
    const shouldStartNewBlock =
      !currentBlock ||
      currentBlock.speaker !== u.speaker ||
      gap > MAX_GAP_SECONDS;

    if (shouldStartNewBlock) {
      if (currentBlock) {
        mergedBlocks.push(currentBlock);
      }
      currentBlock = {
        speaker: u.speaker,
        start: u.start,
        texts: [u.text]
      };
    } else if (currentBlock) {
      currentBlock.texts.push(u.text);
    }

    lastEnd = u.end;
  }

  // Don't forget the last block
  if (currentBlock) {
    mergedBlocks.push(currentBlock);
  }

  // Format merged blocks
  return mergedBlocks
    .map(block => {
      const mins = Math.floor(block.start / 60);
      const secs = Math.floor(block.start % 60);
      const timestamp = `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
      const speakerName = speakerMap.get(block.speaker) || `Speaker ${block.speaker}`;
      const fullText = block.texts.join(' ');
      return `[${timestamp}] [${speakerName}] ${fullText}`;
    })
    .join('\n');
}
